{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c50c1a-cce0-4e82-9c1d-d3d971a3169c",
   "metadata": {},
   "source": [
    "# **_Bank GoodCredit wants to predict credit score for current credit card customers._**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e81756-eac6-4e19-b6d9-e92b9cd462b4",
   "metadata": {},
   "source": [
    "#  Dataset Overview – Bank GoodCredit Credit Risk Project\n",
    "\n",
    "## Business Case:\n",
    "\n",
    "Bank GoodCredit wants to assess the **creditworthiness** of its credit card customers by predicting if a customer is likely to **default** (i.e., become 30+ days past due).\n",
    "This helps reduce credit risk and improve loan decisions.\n",
    "\n",
    "**Target Variable:**\n",
    "`Bad_label`\n",
    "\n",
    "* `0` → Good credit history\n",
    "* `1` → Bad credit history (default risk)\n",
    "\n",
    "---\n",
    "\n",
    "###  **Dataset Description**\n",
    "\n",
    "The data is stored in a **MySQL database** with the following three main tables:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Cust\\_Account**\n",
    "\n",
    "Customer’s historical account and payment information.\n",
    "\n",
    "* `customer_no`\n",
    "* `opened_dt`, `last_paymt_dt`, `closed_dt`\n",
    "* `cur_balance_amt`, `creditlimit`, `cashlimit`\n",
    "* `paymenthistory1`, `paymentfrequency`\n",
    "* `rateofinterest`, `actualpaymentamount`\n",
    "* ... *(more account-specific fields)*\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Cust\\_Enquiry**\n",
    "\n",
    "Contains credit enquiry history for each customer.\n",
    "\n",
    "* `customer_no`\n",
    "* `enquiry_dt`, `enq_purpose`, `enq_amt`\n",
    "* Used to evaluate customer's recent credit-seeking behavior.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Cust\\_Demographics**\n",
    "\n",
    "Includes demographic and anonymized model features.\n",
    "\n",
    "* `customer_no`\n",
    "* `feature_1` to `feature_79`\n",
    "* `Bad_label` (target variable)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project Goal**\n",
    "\n",
    "Build a classification model using this data to:\n",
    "\n",
    "* Analyze customer behavior\n",
    "* Select relevant features\n",
    "* Predict default risk (Bad\\_label)\n",
    "* Evaluate performance using Gini score and decile ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49a780-d5d8-4a10-ab0a-665b1b11e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f001f5a-5c96-4b11-86b4-e9a39018802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Used to interact with the operating system\n",
    "import mysql.connector # Used to connect to MySQL database and load datasets\n",
    "import pandas as pd # Used for data cleaning, manipulation, and analysis\n",
    "import numpy as np # Used for numerical operations and handling arrays\n",
    "import matplotlib.pyplot as plt # Used for data visualization (creating charts and plots)\n",
    "import warnings # Used to manage warning messages\n",
    "import seaborn as sns # Used for statistical data visualization\n",
    "\n",
    "# Ignore warning messages to keep the output clean\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4628123f-2d88-4101-b4d6-d4ba557badbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making connection to the database server\n",
    "connection = mysql.connector.connect(\n",
    "    host=\"18.136.157.135\",  # Database server IP address\n",
    "    user=\"dm_team1\",  # Username for authentication\n",
    "    password=\"DM!$Team&279@20!\"  # Password for authentication\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8e06d-2111-4c10-8beb-60af5bac8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cursor object to interact with the MySQL database\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Execute the SQL command to list all available databases\n",
    "cursor.execute('SHOW DATABASEs')\n",
    "\n",
    "# Print each database name retrieved from the server\n",
    "for db in cursor:\n",
    "    print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb287e-3550-449d-ba4e-5890c1941cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to the MySQL database server\n",
    "connection = mysql.connector.connect(\n",
    "    host=\"18.136.157.135\",  # Database server IP address\n",
    "    port = \"3306\",\n",
    "    user=\"dm_team1\",  # Username for authentication\n",
    "    password=\"DM!$Team&279@20!\",  # Password for authentication\n",
    "    database=\"project_banking\"  # Specify the database to connect to\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761cc2e-d871-4571-a010-0ddcc08abdfe",
   "metadata": {},
   "source": [
    "#### Retrieve all tables from the 'project_banking' database\n",
    "db_Tables = pd.read_sql_query(\"SHOW TABLES\", connection)\n",
    "\n",
    "# Display the list of tables\n",
    "db_Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1df102-8d59-48bd-8385-10873cb67505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all records from the Cust_Account table and store them in a DataFrame\n",
    "customer_acc = pd.read_sql_query('SELECT * FROM Cust_Account',connection)\n",
    "\n",
    "# Display the retrieved data\n",
    "customer_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29656b3-13d3-4eb9-8428-b3876832a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all records from the Cust_Demographics table and store them in a DataFrame\n",
    "customer_demo = pd.read_sql_query('SELECT * FROM Cust_Demographics',connection)\n",
    "\n",
    "# Display the retrieved data\n",
    "customer_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16e5c3-4ed3-4ea5-9bfc-32395fcc5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all records from the Cust_Enquiry table and store them in a DataFrame\n",
    "customer_enq = pd.read_sql_query('SELECT * FROM Cust_Enquiry',connection)\n",
    "\n",
    "# Display the retrieved data\n",
    "customer_enq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3fde5-cedb-4789-bd25-d0e422b8fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the 'customer_acc' DataFrame to a CSV file named 'customer_acc.csv'\n",
    "# Set index=False to avoid writing row numbers (index) into the file\n",
    "customer_acc.to_csv('customer_acc.csv', index=False)\n",
    "\n",
    "# Export the 'customer_demo' DataFrame to a CSV file named 'customer_demo.csv'\n",
    "# This typically contains demographic information like age, gender, etc.\n",
    "customer_demo.to_csv('customer_demo.csv', index=False)\n",
    "\n",
    "# Export the 'customer_enq' DataFrame to a CSV file named 'customer_enq.csv'\n",
    "# This usually includes customer enquiries or interaction history\n",
    "customer_enq.to_csv('customer_enq.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc9100-11a2-46ea-94dc-85f4235f685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('customer_acc.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5a9b9-8190-4069-bb77-20e1cc841830",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('customer_demo.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51954718-920b-432e-87ba-e9639ff3f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('customer_enq.csv')\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be61c0b-af26-431e-8ce2-8bb197c93ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of rows and columns in the  DataFrame\n",
    "# Output will be in the format: (rows, columns)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6571c-6ee1-429e-9a9a-38bff7d12e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of rows and columns in the  DataFrame\n",
    "# Output will be in the format: (rows, columns)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fbca3-fb5f-4c43-b4ed-81bac96d1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of rows and columns in the  DataFrame\n",
    "# Output will be in the format: (rows, columns)\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975b3c6-c409-4b3f-b88b-a532c4ffd4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows in df1 based on the 'customer_no' column\n",
    "# Keeps the first occurrence and drops any additional entries with the same 'customer_no'\n",
    "df1 = df1.drop_duplicates(subset='customer_no')\n",
    "\n",
    "# Remove duplicate rows in df2 based on the 'customer_no' column\n",
    "# Ensures each customer appears only once in the demographic data\n",
    "df2 = df2.drop_duplicates(subset='customer_no')\n",
    "\n",
    "# Remove duplicate rows in df3 based on the 'customer_no' column\n",
    "# Cleans up the enquiry data by removing repeated entries for the same customer\n",
    "df3 = df3.drop_duplicates(subset='customer_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb25af-428e-49b5-9eaa-b698423407c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df1 (customer account data) and df3 (customer enquiry data) using an inner join on 'customer_no'\n",
    "# This keeps only the rows where 'customer_no' exists in both df1 and df3\n",
    " # Then merge the result with df2 (customer demographic data), again using an inner join on 'customer_no'\n",
    "          # This ensures the final DataFrame 'data' contains only those customers present in all three DataFrames\n",
    "data = df1.merge(df3, how=\"inner\", on=\"customer_no\").merge(df2, how=\"inner\", on=\"customer_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3634ffe5-7d53-471f-8b71-dbda643c5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display merged DataFrame\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7870388a-2841-4b09-8e7b-a6c1438e82d3",
   "metadata": {},
   "source": [
    "## Basic Check :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08d2ab-9dcb-421f-8309-4ed5de9a41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()  #showing first 5 rows of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d178cf0-d292-4be1-85e6-68368ad16ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()  # showung last 5 rows of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25255b8-e1a9-4a2a-a826-66802551954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=\"O\") # showing categorical column's information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03e130-f195-464f-bbc0-a6779643b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()#showing information about all columns of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a468b5-0c37-4d23-ac6e-8875d73e10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a9c14-84c1-4a7f-b60b-7f559604c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb80aef-ed40-4b0d-b0f8-5ed2320c6bf6",
   "metadata": {},
   "source": [
    "## Basic Data Preprocessing :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2a435-1562-4bbc-9f84-be21d9dcce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = [\n",
    "    'dt_opened_x', 'upload_dt_x', 'last_paymt_dt',\n",
    "    'reporting_dt', 'paymt_str_dt', 'paymt_end_dt', 'dt_opened_y', 'entry_time',\n",
    "    'feature_2', 'feature_21', 'feature_53', 'feature_54','dt_opened', \n",
    "    'upload_dt_y', 'enquiry_dt','feature_30','feature_39','feature_63','feature_75']\n",
    "\n",
    "\n",
    "cat_col = [\n",
    "    'paymenthistory1', 'paymenthistory2', 'feature_1', 'feature_5', 'feature_8',\n",
    "    'feature_9', 'feature_11', 'feature_12', 'feature_13', 'feature_15', 'feature_16',\n",
    "    'feature_18', 'feature_20', 'feature_22', 'feature_23', 'feature_24', 'feature_27',\n",
    "    'feature_28', 'feature_32', 'feature_33', 'feature_36', 'feature_37', 'feature_38',\n",
    "    'feature_43', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_50',\n",
    "    'feature_77', 'feature_51', 'feature_57', 'feature_58', 'feature_59', 'feature_60',\n",
    "    'feature_61','feature_62', 'feature_72', 'feature_73', 'feature_79'\n",
    "]\n",
    "\n",
    "\n",
    "num_col = [\n",
    "    'customer_no', 'acct_type', 'owner_indic', 'high_credit_amt', 'cur_balance_amt',\n",
    "    'amt_past_due', 'creditlimit', 'cashlimit', 'rateofinterest', 'paymentfrequency',\n",
    "    'actualpaymentamount', 'feature_3', 'feature_4', 'feature_6', 'feature_7',\n",
    "    'feature_10', 'feature_14', 'feature_17', 'feature_19', 'feature_25', 'feature_26',\n",
    "    'feature_29', 'feature_35', 'feature_40', 'feature_41', 'feature_42',\n",
    "    'feature_44', 'feature_52', 'feature_55', 'feature_56', 'feature_64', 'feature_65',\n",
    "    'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_71', 'feature_74',\n",
    "    'Bad_label', 'feature_34', 'feature_31', 'enq_purpose', 'enq_amt',\n",
    "    'feature_49', 'feature_76', 'feature_78'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1294fcd7-0589-415e-a511-4cdcb9865302",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70d1cb-8fba-42ed-b7df-7d201462d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each column name in the list 'cat_col'\n",
    "# These are assumed to be categorical columns in the DataFrame\n",
    "for i in cat_col:\n",
    "    \n",
    "    # Convert the data type of each column to 'object' (which is used for strings/categorical data in pandas)\n",
    "    data[i] = data[i].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6d614-6710-4d1f-9967-c2747bc41141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each column name in the list 'num_col'\n",
    "# These are assumed to be numerical columns in the DataFrame\n",
    "for i in num_col:\n",
    "    \n",
    "    # Convert each column to numeric type using pandas 'pd.to_numeric'\n",
    "    # The 'errors='coerce'' argument replaces any non-numeric values with NaN (Not a Number)\n",
    "    data[i] = pd.to_numeric(data[i], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4f120-a4d2-4408-a152-32e73ed44539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'opened_dt' column to a datetime format using 'datetime64[ns]'\n",
    "# This ensures that the 'opened_dt' column is treated as a datetime type in pandas\n",
    "data[\"opened_dt\"] = data[\"opened_dt\"].astype(\"datetime64[ns]\")\n",
    "\n",
    "# Convert the 'closed_dt' column to a datetime format using 'datetime64[ns]'\n",
    "# This ensures that the 'closed_dt' column is treated as a datetime type in pandas\n",
    "data[\"closed_dt\"] = data[\"closed_dt\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63571e-b1fa-456f-8df4-9f0cafa1b975",
   "metadata": {},
   "source": [
    "# Feature Engineering :-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dddf226-e338-4fba-93f2-14558ff9c65f",
   "metadata": {},
   "source": [
    "> * feature engineering is the process of creating new features or transforming existing ones to improve model performance. Below are engineered features for credit risk analysis based on account history, enquiry records, and demographic data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f997939-d684-46c8-a6ee-1a91b3dd419f",
   "metadata": {},
   "source": [
    "##  Data Preprocessing Overview\n",
    "\n",
    "Data preprocessing prepares raw data for modeling by cleaning inaccuracies, handling missing values, and encoding categorical features. It also involves scaling numerical data and selecting relevant features. This process ensures data quality, enhances model performance, and improves training efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### **Data Cleaning**\n",
    "\n",
    "Removing or fixing inaccuracies in the dataset, such as:\n",
    "\n",
    "* Duplicate records\n",
    "* Inconsistent formatting\n",
    "* Irrelevant or redundant columns\n",
    "\n",
    "---\n",
    "\n",
    "### **Handling Missing Values**\n",
    "\n",
    "Missing data can negatively impact models. Common strategies include:\n",
    "\n",
    "* Filling with statistical values like **mean**, **median**, or **mode**\n",
    "* Using placeholders (e.g., `\"Unknown\"` or `-999`)\n",
    "* Dropping rows or columns with excessive missing values\n",
    "\n",
    "---\n",
    "\n",
    "### **Encoding Categorical Variables**\n",
    "\n",
    "Machine learning algorithms require numerical input. Categorical data like `\"Gender\"` or `\"City\"` must be converted using:\n",
    "\n",
    "* **Label Encoding** – for ordinal categories\n",
    "* **One-Hot Encoding** – for nominal categories\n",
    "\n",
    "---\n",
    "\n",
    "### **Scaling and Normalization**\n",
    "\n",
    "To ensure that all features contribute equally to the model:\n",
    "\n",
    "* **Normalization** scales values between 0 and 1\n",
    "  \n",
    "---\n",
    "\n",
    "### **Feature Selection and Extraction**\n",
    "\n",
    "Improving model performance by:\n",
    "\n",
    "* **Feature Selection**: Keeping only the most relevant features based on statistical tests or model-based importance\n",
    "* **Feature Extraction**: Creating new features (e.g., combining year and month into a time series index) that reveal hidden patterns or reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f3ad3-c5df-4292-83bf-d28fd4c64183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check null values only in specified categorical columns\n",
    "cat_null = data[cat_col].isnull().sum()\n",
    "\n",
    "# Display only columns that actually have null values\n",
    "cat_null[cat_null > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844bbf1e-700e-4311-bf3a-5245c82606a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check null values only in specified categorical columns\n",
    "num_null = data[num_col].isnull().sum()\n",
    "\n",
    "# Display columns\n",
    "num_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c16ed-7143-4348-a768-4586ecf1366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_null.index:\n",
    "    mode_value = data[col].mode()[0]  # Get the mode (most frequent value)\n",
    "    data[col].fillna(mode_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ab51d-5fbc-4e06-9162-cc6d4a35869e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check null values only in specified categorical columns\n",
    "cat_null = data[cat_col].isnull().sum()\n",
    "\n",
    "# Display only columns that actually have null values\n",
    "cat_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce8fc8-f09e-46a6-adbe-ca2beacb669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_null.index:\n",
    "    median_value = data[col].median()  # Get median of the column\n",
    "    data[col].fillna(median_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e2776-0056-4382-9b19-a94d77566ff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check null values only in specified categorical columns\n",
    "cat_null = data[cat_col].isnull().sum()\n",
    "\n",
    "# Display only columns that actually have null values\n",
    "cat_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723052f-4837-49f8-a7a4-3e2c36e79e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e4ecf-caaf-4e3c-a0e6-b25fbaaf5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns listed in 'date_col' from the DataFrame 'data'\n",
    "# 'axis=1' specifies that columns (not rows) should be dropped\n",
    "# 'inplace=True' modifies the 'data' DataFrame directly without needing to assign the result to a new variable\n",
    "data.drop(date_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e8159-1b30-4bf4-8854-ac151801154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mode (most frequent value) of the 'closed_dt' column\n",
    "# The mode is useful for understanding the most common date in the 'closed_dt' column\n",
    "# It returns a pandas Series, so you can access the first mode value using [0] if needed\n",
    "data['closed_dt'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea318d-f3db-407a-838d-73e99d1fa808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mode (most frequent value) of the 'opened_dt' column\n",
    "# The mode is useful for understanding the most common date in the 'opened_dt' column\n",
    "# It returns a pandas Series, so you can access the first mode value using [0] if needed\n",
    "data['opened_dt'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699aa6a-f5e2-4c3e-a2f7-4a2b8308fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing (null) values in the 'closed_dt' column with a specific date ('2015-02-04')\n",
    "# 'data['closed_dt'].isnull()' identifies rows where 'closed_dt' is null\n",
    "# 'data.loc' is used to update the values in these rows, setting them to the specified date\n",
    "data.loc[data['closed_dt'].isnull(), 'closed_dt'] = '2015-02-04'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5447e5c-47c2-491f-bb17-c5c0f483e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing (null) values in the 'opened_dt' column with a specific date ('2015-03-31')\n",
    "# 'data['opened_dt'].isnull()' identifies rows where 'opened_dt' is null\n",
    "# 'data.loc' is used to update the values in these rows, setting them to the specified date\n",
    "data.loc[data['opened_dt'].isnull(),'opened_dt'] = '2015-03-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41d635-23da-4325-8913-82488f4d9c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbec43b-7321-4966-8728-8b35b77facf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop specific columns from the 'data' DataFrame\n",
    "# The list inside the 'drop()' function contains the column names to be removed\n",
    "# 'axis=1' specifies that columns (not rows) should be dropped\n",
    "# 'inplace=True' modifies the original 'data' DataFrame directly without creating a new DataFrame\n",
    "data.drop(['customer_no', 'paymenthistory1', 'paymenthistory2', 'feature_10', 'feature_18', \n",
    "           'feature_20', 'feature_22', 'feature_24', 'feature_45', 'feature_47', 'feature_49', \n",
    "           'feature_61', 'feature_77'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a5371-2d74-4fc5-bb27-e2e4d65e8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns from the 'data' DataFrame that have the data type 'object'\n",
    "# 'object' type typically represents categorical or string data in pandas\n",
    "# 'select_dtypes(include=['object'])' filters columns with 'object' type\n",
    "# '.columns' retrieves the column names of the selected columns\n",
    "# '.tolist()' converts the column names into a list and stores it in 'cat_col'\n",
    "cat_col = data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040cf3b-8bc1-4567-9c29-d85937c5a5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns from the 'data' DataFrame that have numeric data types: 'float64' or 'int64'\n",
    "# 'float64' and 'int64' represent continuous and discrete numeric values, respectively\n",
    "# 'select_dtypes(include=['float64', 'int64'])' filters columns with numeric types\n",
    "# '.columns' retrieves the column names of the selected columns\n",
    "# '.tolist()' converts the column names into a list and stores it in 'num_col'\n",
    "num_col = data.select_dtypes(include=['float64', 'int64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23330dc0-7ad7-416b-9a82-17740de923df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure 'Bad_label' is numeric\n",
    "data['Bad_label'] = pd.to_numeric(data['Bad_label'], errors='coerce')\n",
    "\n",
    "# Step 2: Filter only numeric columns from the DataFrame\n",
    "num_data = data[num_col].select_dtypes(include=['number'])\n",
    "\n",
    "# Step 3: Add 'Bad_label' to the numeric data\n",
    "num_data['Bad_label'] = data['Bad_label']\n",
    "\n",
    "# Step 4: Compute correlation matrix\n",
    "corr_matrix = num_data.corr()\n",
    "\n",
    "# Step 5: Extract correlation of all features with 'Bad_label' (excluding itself)\n",
    "corr_series = corr_matrix['Bad_label'].drop('Bad_label')\n",
    "\n",
    "# Step 6: Sort by absolute correlation\n",
    "corr_sorted = corr_series.abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785074aa-f2a1-4549-a385-66df7c752dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each numeric column in the 'num_col' list\n",
    "for i in num_col:\n",
    "    \n",
    "    # If the column is in the 'corr_sorted' list (i.e., has a very weak correlation with 'Bad_label'), skip it\n",
    "    if i in corr_sorted:\n",
    "        continue  # Skip the column and move to the next iteration of the loop\n",
    "    \n",
    "    # If the column is not in the 'corr_sorted' list, print the column name\n",
    "    else:\n",
    "        print(i)  # This will display columns that have a moderate to strong correlation with 'Bad_label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88d921-3921-4ba3-9495-7f2071259bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083af3ce-3141-4e60-b6d0-a627045f20d2",
   "metadata": {},
   "source": [
    "# EDA :- Exploratory Data Analysis\n",
    "### Univarinat Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2405d5c-acb6-49bf-81e9-fec796dc9b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "data['Bad_label'].value_counts(normalize=True)\n",
    "data['Bad_label'].value_counts().plot(kind='bar', title='Target Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e217414-cbf8-4011-9c3f-fb0e0025483f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For numeric\n",
    "for col in num_col:\n",
    "    plt.figure()\n",
    "    sns.histplot(data[col], kde=True)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.show()\n",
    "\n",
    "# For categorical\n",
    "for col in cat_col:\n",
    "    plt.figure()\n",
    "    data[col].value_counts().plot(kind='bar')\n",
    "    plt.title(f\"Count plot of {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a53219-b2fa-4629-8ee7-e056122ea763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of the entire figure (25x35) for better spacing between subplots\n",
    "plt.figure(figsize=(25, 35))  \n",
    "\n",
    "# Initialize a counter to manage subplot positions\n",
    "num = 1\n",
    "\n",
    "# Loop through each categorical column in the 'cat_col' list\n",
    "for i in cat_col:\n",
    "    \n",
    "    # Only plot columns with less than or equal to 50 unique values (low-cardinality)\n",
    "    if data[i].nunique() <= 50:\n",
    "        \n",
    "        # Create a subplot with a grid of 6 rows and 5 columns to fit multiple plots\n",
    "        plt.subplot(6, 5, num)\n",
    "        \n",
    "        # Plot a histogram with a KDE (Kernel Density Estimate) for the categorical feature 'i'\n",
    "        # 'sns.histplot()' shows the distribution, and 'kde=True' adds the KDE curve\n",
    "        sns.histplot(data=data, x=i, kde=True, color='skyblue')\n",
    "        \n",
    "        # Plot a count plot (bar plot) for the categorical feature 'i'\n",
    "        # 'sns.countplot()' shows the frequency of each category in the feature\n",
    "        # 'palette=\"viridis\"' sets the color scheme\n",
    "        # 'order=data[i].value_counts().index' ensures the categories are ordered by their frequency\n",
    "        sns.countplot(data=data, x=i, palette='viridis', order=data[i].value_counts().index)\n",
    "        \n",
    "        # Set the x-axis label to the column name with a larger font size for better readability\n",
    "        plt.xlabel(i, fontsize=14)\n",
    "        \n",
    "        # Rotate the x-axis labels by 45 degrees and align them to the right for better readability\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Adjust the layout of the plots to prevent overlapping and ensure proper spacing between subplots\n",
    "        plt.tight_layout(pad=2)  \n",
    "        \n",
    "        # Increment the subplot number to move to the next subplot position\n",
    "        num += 1  \n",
    "\n",
    "# Show the plots after all subplots are created\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734045ff-79c9-4407-9118-af18f5e6a6ab",
   "metadata": {},
   "source": [
    "##  Feature Distribution Summary & Recommendations\n",
    "\n",
    "###  Well-distributed Features\n",
    "These features show a good spread across categories and are likely useful for modeling:\n",
    "- `feature_1`\n",
    "- `feature_27`\n",
    "- `feature_32`\n",
    "- `feature_33`\n",
    "- `feature_50`\n",
    "\n",
    "###  Highly Imbalanced Features\n",
    "These features are heavily skewed toward one or very few values. They may provide limited information and require further evaluation:\n",
    "- `feature_11`\n",
    "- `feature_13`\n",
    "- `feature_23`\n",
    "- `feature_57`\n",
    "- `feature_59`\n",
    "- `feature_60`\n",
    "- `feature_62`\n",
    "- `feature_72`\n",
    "- `feature_73`\n",
    "\n",
    "###  High Cardinality with Sparse Values\n",
    "These features have many unique values with low frequency. They may increase model complexity and should be encoded or reduced:\n",
    "- `feature_8`\n",
    "- `feature_9`\n",
    "- `feature_37`\n",
    "- `feature_51`\n",
    "\n",
    "### Constant or Nearly Constant Feature (Drop)\n",
    "These features have no or very little variance and should be dropped:\n",
    "- `feature_79`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62090f6-55ff-4378-aaab-7ecc319af1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Boxplot for numerical vs target\n",
    "for col in num_col:\n",
    "    plt.figure()\n",
    "    sns.boxplot(x='Bad_label', y=col, data=data)\n",
    "    plt.title(f\"{col} vs Bad_label\")\n",
    "    plt.show()\n",
    "\n",
    "# Countplot for categorical vs target\n",
    "for col in cat_col:\n",
    "    plt.figure()\n",
    "    sns.countplot(x=col, hue='Bad_label', data=data)\n",
    "    plt.title(f\"{col} vs Bad_label\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00caf2d1-f01a-42ff-83c1-85a305824ca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# Define number of columns per row in the grid\n",
    "n_cols = 5\n",
    "\n",
    "# Total number of plots\n",
    "n_plots = len(num_col)\n",
    "\n",
    "# Calculate rows needed\n",
    "n_rows = math.ceil(n_plots / n_cols)\n",
    "\n",
    "# Set figure size based on number of rows/columns\n",
    "plt.figure(figsize=(n_cols * 4, n_rows * 3))\n",
    "\n",
    "# Loop through and create subplots\n",
    "for idx, col in enumerate(num_col):\n",
    "    plt.subplot(n_rows, n_cols, idx + 1)\n",
    "    sns.boxplot(y=data[col], color='skyblue')\n",
    "    plt.title(col, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle(\"Boxplots for Outlier Detection\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b630620-d2fb-4315-8386-d8eedaa2d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of columns that are suspected or known to contain outliers in the 'data' DataFrame\n",
    "# These columns may have extreme values or deviations that could affect analysis or modeling\n",
    "outliers_col =  [ \n",
    "    \"acct_type\",\"high_credit_amt\",\"cur_balance_amt\",\"amt_past_due\",\"creditlimit\",\"cashlimit\",\"enq_purpose\",\"enq_amt\",\n",
    "    \"feature_3\",\"feature_26\",\"feature_56\",\"feature_66\",\"feature_71\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aced5a-b284-4a2c-8c13-88acfc95a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each column in the 'outliers_col' list to handle outliers\n",
    "for i in outliers_col:\n",
    "    \n",
    "    # Calculate the first quartile (Q1) of the column 'i' using the quantile function\n",
    "    Q1 = data[i].quantile(0.25)\n",
    "    \n",
    "    # Calculate the third quartile (Q3) of the column 'i'\n",
    "    Q3 = data[i].quantile(0.75)\n",
    "    \n",
    "    # Calculate the Interquartile Range (IQR) which is the difference between Q3 and Q1\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define the lower bound for outliers as Q1 - 1.5 * IQR\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    \n",
    "    # Define the upper bound for outliers as Q3 + 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Replace values in the column 'i' that are below the lower bound or above the upper bound with the median of the column\n",
    "    # This helps to handle outliers by replacing them with a value closer to the center of the data\n",
    "    data.loc[(data[i] > upper_bound) | (data[i] < lower_bound), i] = data[i].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d8a02-6e19-462f-a76b-92ba323538be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size for the entire plot (20x25) to ensure ample space for multiple subplots\n",
    "plt.figure(figsize=(20, 25))\n",
    "\n",
    "# Initialize a counter variable 'num' to manage the subplot positions\n",
    "num = 1\n",
    "\n",
    "# Loop through each numeric column in the 'num_col' list\n",
    "for i in num_col:\n",
    "    \n",
    "    # Ensure the loop runs only if 'num' is less than or equal to the total number of numeric columns\n",
    "    if num <= len(num_col):\n",
    "        \n",
    "        # Create a subplot with a 9x9 grid layout, where 'num' determines the position of each plot\n",
    "        plt.subplot(9, 9, num)\n",
    "        \n",
    "        # Plot a boxplot for the current numeric column 'i'\n",
    "        # Boxplots help visualize the distribution and identify outliers for each numeric feature\n",
    "        sns.boxplot(x=data[i]) \n",
    "        \n",
    "        # Set the x-axis label for each subplot to the current column name\n",
    "        plt.xlabel(i, fontsize=20)\n",
    "        \n",
    "        # Increment the 'num' variable to move to the next subplot position\n",
    "        num += 1 \n",
    "\n",
    "# Adjust the layout to ensure the subplots don't overlap and are spaced neatly\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display all the generated subplots (boxplots)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9badf6f5-f818-4672-a94c-554c173f893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas' 'get_dummies' function to convert categorical column 'feature_1' into dummy/indicator variables\n",
    "# This is a common step for one-hot encoding categorical variables, where each unique category in 'feature_1' gets its own column\n",
    "# The 'dtype=int' ensures the dummy variables are of integer type (0 or 1) instead of the default float type.\n",
    "data = pd.get_dummies(data, columns=['feature_1'], dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffef8f7-03db-4617-a81f-bccb19b9bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'opened_dt' column to datetime format using pandas' 'to_datetime' function\n",
    "# This ensures the column is treated as a datetime object, allowing for time-based operations\n",
    "data['opened_dt'] = pd.to_datetime(data['opened_dt'])\n",
    "\n",
    "# Extract the year, month, and day from the 'opened_dt' datetime column and create new columns\n",
    "# 'data['year']' will store the year part of the 'opened_dt'\n",
    "# 'data['month']' will store the month part of the 'opened_dt'\n",
    "# 'data['day']' will store the day part of the 'opened_dt'\n",
    "data['year'], data['month'], data['day'] = data['opened_dt'].dt.year, data['opened_dt'].dt.month, data['opened_dt'].dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87b4b4-8cd0-4bd2-a7fd-75877467daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values (NaNs) in the 'year' column with the most frequent value (mode) of that column\n",
    "# The 'data['year'].mode()[0]' returns the most frequent value (the mode) of the 'year' column\n",
    "# 'inplace=True' ensures that the changes are applied directly to the 'year' column in the 'data' DataFrame without creating a new variable\n",
    "data['year'].fillna(data['year'].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164bb325-3813-4898-886a-159d11494475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values (NaNs) in the 'month' column with the most frequent value (mode) of that column\n",
    "# The 'data['month'].mode()[0]' returns the most frequent value (the mode) of the 'month' column\n",
    "# 'inplace=True' ensures that the changes are applied directly to the 'month' column in the 'data' DataFrame without creating a new variable\n",
    "data['month'].fillna(data['month'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03936f7a-8016-401e-95a3-55b8b855697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values (NaNs) in the 'month' column with the most frequent value (mode) of that column\n",
    "# The 'data['month'].mode()[0]' returns the most frequent value (the mode) of the 'month' column\n",
    "# 'inplace=True' ensures that the changes are applied directly to the 'month' column in the 'data' DataFrame without creating a new variable\n",
    "data['month'].fillna(data['month'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191a22a-b29b-4769-86c1-474a9a64e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values (NaNs) in the 'day' column with the most frequent value (mode) of that column\n",
    "# The 'data['day'].mode()[0]' returns the most frequent value (the mode) of the 'day' column\n",
    "# 'inplace=True' ensures that the changes are applied directly to the 'day' column in the 'data' DataFrame without creating a new variable\n",
    "data['day'].fillna(data['day'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b83a0f-20ff-44a7-8366-48a3a70c743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'closed_dt' column to datetime format using pandas' 'to_datetime' function\n",
    "# This ensures the column is treated as a datetime object, allowing for time-based operations\n",
    "data['closed_dt'] = pd.to_datetime(data['closed_dt'])\n",
    "\n",
    "# Extract the year, month, and day from the 'closed_dt' datetime column and create new columns\n",
    "# 'data['year']' will store the year part of the 'closed_dt'\n",
    "# 'data['month']' will store the month part of the 'closed_dt'\n",
    "# 'data['day']' will store the day part of the 'closed_dt'\n",
    "data['year_close'], data['month_close'], data['day_close'] = data['closed_dt'].dt.year, data['closed_dt'].dt.month, data['closed_dt'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b8aa41-0f55-4f4a-b448-2fe31bf6ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values (NaNs) in the 'year_close' column with the most frequent value (mode) of that column\n",
    "# The 'data['year_close'].mode()[0]' returns the most frequent value (the mode) of the 'year_close' column\n",
    "# 'inplace=True' ensures that the changes are applied directly to the 'year_close' column in the 'data' DataFrame without creating a new variable\n",
    "data['year_close'].fillna(data['year_close'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61666ad3-f878-42ce-8168-f287b75b3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values (NaNs) in the 'month_close' column with the most frequent value (mode) of that column\n",
    "# The 'data['month_close'].mode()[0]' returns the most frequent value (the mode) of the 'month_close' column\n",
    "# 'inplace=True' ensures that the changes are applied directly to the 'month_close' column in the 'data' DataFrame without creating a new variable\n",
    "data['month_close'].fillna(data['month_close'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2009ac3-9ec5-4e94-a01e-47b4f18161f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'opened_dt' and 'closed_dt' columns from the DataFrame as they are no longer needed for analysis\n",
    "# The 'axis=1' argument specifies that we are dropping columns (as opposed to rows)\n",
    "# 'inplace=True' ensures that the changes are applied directly to the 'data' DataFrame without creating a new one\n",
    "data.drop(['opened_dt', 'closed_dt'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6773f-e236-4e11-9faf-d48f35151032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns from the DataFrame that have data types of 'object' (i.e., categorical or string columns)\n",
    "# 'data.select_dtypes(include=['object'])' returns all columns that are of type 'object', which typically represents categorical variables\n",
    "# '.columns.tolist()' extracts the column names from the resulting DataFrame and converts them into a list\n",
    "# This will give a list of all the categorical columns in the 'data' DataFrame\n",
    "cat_col = data.select_dtypes(include=['object']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb777525-543a-4e85-9992-468bf3696d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns from the DataFrame that have data types of 'object' (i.e., categorical or string columns)\n",
    "# 'data.select_dtypes(include=['object'])' returns all columns that are of type 'object', which typically represents categorical variables\n",
    "# '.columns.tolist()' extracts the column names from the resulting DataFrame and converts them into a list\n",
    "# This will give a list of all the categorical columns in the 'data' DataFrame\n",
    "cat_col = data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99f679-df78-4c6a-bfa0-091c45eab4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1daff-ad40-4a89-a0bf-b73ccbc0aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83374047-a48a-4113-a98c-efbbefe1a0e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769b75c-9572-484f-aa99-84a5d65d57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the LabelEncoder class from scikit-learn's preprocessing module\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "# Initialize a LabelEncoder object (LE) that will be used to encode categorical labels into numeric values\n",
    "LE = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to each categorical column in the 'cat_col' list\n",
    "# The 'fit_transform()' method learns the mapping of each category to a numeric value and then transforms the column to numeric values\n",
    "for i in cat_col:\n",
    "    data[i] = LE.fit_transform(data[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237cad2-004e-4adc-a46b-bd1b3a4e7334",
   "metadata": {},
   "source": [
    "### Feature Selection :-\n",
    "* Feature selection is the process of choosing the most relevant variables from a dataset that contribute significantly to a model’s predictive power. By reducing the number of features, it simplifies the model, improves training efficiency, and can enhance accuracy by removing noise or irrelevant data. Common techniques include correlation analysis, mutual information, and advanced methods like recursive feature elimination (RFE) and Lasso regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace79fa-8456-418f-a692-23f6356df221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns with numerical data types (int and float) from the DataFrame using 'np.number'\n",
    "num_cols = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute the correlation matrix for the numerical columns using the '.corr()' method\n",
    "# The '.corr()' method calculates pairwise correlation of columns (default is Pearson correlation)\n",
    "# The '.abs()' function is used to get the absolute values of the correlation coefficients, as we are interested in the magnitude of the correlation, regardless of the sign\n",
    "correlation_matrix = num_cols.corr().abs()\n",
    "\n",
    "# Iterate through the upper triangle of the correlation matrix to find pairs of columns with high correlation\n",
    "# The outer loop iterates through the rows of the correlation matrix\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    # The inner loop iterates through the columns before the current row (avoiding duplicate checks)\n",
    "    for j in range(i):\n",
    "        # Get the correlation value between the pair of columns\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        # If the absolute correlation value is greater than 0.90, print the pair of columns with high correlation\n",
    "        if abs(corr_value) > 0.90:\n",
    "            # Print the pair of columns and their correlation coefficient\n",
    "            print(f\"High correlation: {correlation_matrix.columns[i]} ↔ {correlation_matrix.columns[j]} (r = {corr_value:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe4d15-4a35-4ea2-ade6-0ddbe5128949",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['feature_9','feature_29','feature_55','feature_59','feature_67','feature_68','feature_69','feature_34','feature_68' ],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d84cbb-df4b-416c-84b9-988b81c59b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8d1bf-6cd2-471f-ae9c-ca8b1e1d00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b376a729-8bb0-46c3-a03f-6f0208da5b01",
   "metadata": {},
   "source": [
    "### Model Creation Process :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498fc1e-4363-4c67-bd48-cd4a57fd7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(\"Bad_label\",axis=1)\n",
    "y = data.Bad_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2468b12-a678-4185-b545-bf52e6dccbc0",
   "metadata": {},
   "source": [
    "### Scalling Technique:-\n",
    "* Scaling is very important of continous features because some model give more priproty to that features which has higest values campare to another features. that's why we use Scaling to make all values into certain range.\n",
    " \n",
    "* Basically there are 2 type of scaling.\n",
    "\n",
    "1. **MinMax Scaling**\n",
    "2. **Standardization Scaling**\n",
    "\n",
    "* MinMax Scaling transform all data between 0 to 1  range.\n",
    "* Standardization Scaling transform all data between -3 to +3 range. it use z-score to tranform data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b62f0e2-ee64-48e5-ae5c-0b8115dc5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the StandardScaler class from scikit-learn's preprocessing module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler object, which will be used to standardize the features\n",
    "# StandardScaler standardizes features by removing the mean and scaling to unit variance (z-score normalization)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f5b55-2ff5-416c-8001-10caf0fc0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the feature set 'x' using the 'scaler' object (StandardScaler)\n",
    "# 'fit_transform()' first computes the mean and standard deviation of each feature in 'x' (fit),\n",
    "# and then scales the data by transforming it (subtracting the mean and dividing by the standard deviation)\n",
    "# The result is stored in 'x_scale', which is the standardized version of the input features 'x'\n",
    "x_scale = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202139f5-f904-499a-9c79-1f0a2d516fef",
   "metadata": {},
   "source": [
    "## PCA:- Principle Components Analysis\n",
    "\n",
    "* Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a smaller set of uncorrelated variables called principal components. These components capture the most variance in the data, making it possible to reduce complexity while retaining important information. PCA is useful for visualizing data in lower dimensions and improving model performance by removing noise and redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ce00d-6455-4f2a-94c7-f9b536e2a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the PCA (Principal Component Analysis) class from scikit-learn's decomposition module\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize the PCA object, specifying the number of components to keep after dimensionality reduction\n",
    "# 'n_components=15' means that PCA will reduce the data to 15 principal components, \n",
    "# which are the new features that capture the most variance in the data\n",
    "pca = PCA(n_components=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5022b-f0f8-4e03-a1da-9068c9c9da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA (Principal Component Analysis) to the scaled feature set 'x_scale'\n",
    "# The 'fit_transform()' method first computes the principal components (fit) and then transforms the data (x_scale) \n",
    "# into a lower-dimensional space using the top 15 principal components as specified earlier.\n",
    "# The result is stored in 'data_pca', which is the transformed dataset with reduced dimensions (15 components).\n",
    "data_pca = pca.fit_transform(x_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ea94e-a4a1-4722-930b-a5e31312bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd32106-2ca8-4d5f-8489-c0bea8af0a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea1112-e096-44f8-82b4-4bd707b6b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(data_pca,y,test_size=0.25,random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c2b20-cbf7-4edc-ac96-c6f21ce84af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7e4eb-a2fb-4ee6-996b-0e60d1d46c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47a599-0072-4076-becb-e1285d7aaf36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703384b-9dce-4ada-9df4-49c4b66300bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c34911-be5e-4b86-aaf3-4f1c98796b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing various classification models from scikit-learn, CatBoost, and XGBoost\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression, used for binary or multiclass classification\n",
    "from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors, a simple algorithm for classification based on proximity to neighbors\n",
    "from sklearn.svm import SVC, LinearSVC  # Support Vector Classifier (SVC), used for classification tasks with high-dimensional data; LinearSVC is for linear decision boundaries\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision Tree, a non-linear classifier that splits data based on feature values\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier  # Ensemble methods for improved performance:\n",
    "# RandomForest (multiple decision trees), Gradient Boosting (sequential learning), and AdaBoost (boosting weak learners)\n",
    "from sklearn.neural_network import MLPClassifier  # Multi-layer Perceptron (MLP), a type of neural network for classification\n",
    "from catboost import CatBoostClassifier  # CatBoost, a gradient boosting algorithm designed for categorical data\n",
    "from xgboost import XGBClassifier  # XGBoost, a highly efficient gradient boosting library for classification and regression tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37aae4-c809-49e0-9695-5e74ec816cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing classification evaluation metrics: confusion_matrix, accuracy_score, precision_score, \n",
    "# classification_report, f1_score, and roc_auc_score to assess model performance across various metrics.\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,classification_report,f1_score,roc_auc_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc6838-a4fd-4c99-9ae9-6192860a5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a dictionary of classification models with their respective hyperparameters\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),  # Logistic Regression with max iterations set to 1000\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),  # K-Nearest Neighbors classifier\n",
    "    'Support Vector Machine': SVC(),  # Support Vector Machine classifier\n",
    "    'Linear SVM': LinearSVC(max_iter=10000),  # Linear Support Vector Machine with max iterations set to 10000\n",
    "    'Decision Tree': DecisionTreeClassifier(),  # Decision Tree classifier\n",
    "    'Random Forest': RandomForestClassifier(),  # Random Forest classifier (ensemble method)\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),  # Gradient Boosting classifier (ensemble method)\n",
    "    'AdaBoost': AdaBoostClassifier(),  # AdaBoost classifier (ensemble method)\n",
    "    'MLP Classifier': MLPClassifier(max_iter=1000),  # Multi-layer Perceptron classifier with max iterations set to 1000\n",
    "    'CatBoost': CatBoostClassifier(verbose=0),  # CatBoost classifier with verbosity turned off\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss')  # XGBoost classifier with settings for label encoding and evaluation metric\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda250ab-3592-4208-a2d7-e82915bdc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing evaluation metrics for classification model performance\n",
    "a = accuracy_score  # Accuracy metric\n",
    "p = precision_score  # Precision metric\n",
    "f = f1_score  # F1 score metric\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(true, predicted):\n",
    "    accuracy_score = a(true, predicted)  # Calculate accuracy\n",
    "    precision_score = p(true, predicted)  # Calculate precision\n",
    "    f1_score_score = f(true, predicted)  # Calculate F1 score\n",
    "    auc = roc_auc_score(true, predicted)  # Calculate ROC AUC score\n",
    "    gini = 2 * auc - 1  # Calculate Gini coefficient (derived from AUC)\n",
    "    return accuracy_score, precision_score, f1_score_score, (gini * 100)  # Return evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae5976-75c5-4aaf-a7ac-11857cc7fd51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store model names and their corresponding accuracy scores\n",
    "model_list = []  # List to store model names\n",
    "accuracy_score_list = []  # List to store model test accuracy scores\n",
    "\n",
    "# Step 3: Loop through models and evaluate their performance\n",
    "for model_name, model in models.items():\n",
    "    model.fit(x_train, y_train)  # Train the model using the training data\n",
    "\n",
    "    # Predictions on training and test sets\n",
    "    y_train_pred = model.predict(x_train)  # Predict on training data\n",
    "    y_test_pred = model.predict(x_test)  # Predict on test data\n",
    "\n",
    "    # Evaluate model performance on both training and test sets\n",
    "    model_train_accuracy_score, model_train_precision_score, model_train_f1_score, model_train_gini = evaluate_model(y_train, y_train_pred)\n",
    "    model_test_accuracy_score, model_test_precision_score, model_test_f1_score, model_test_gini = evaluate_model(y_test, y_test_pred)\n",
    "\n",
    "    # Print and store the results for both training and test sets\n",
    "    print(f\"Model: {model_name}\")\n",
    "    model_list.append(model_name)  # Store model name\n",
    "\n",
    "    print('Model Performance for Training Set:')\n",
    "    print(\"- Accuracy: {:.4f}\".format(model_train_accuracy_score))\n",
    "    print(\"- Precision: {:.4f}\".format(model_train_precision_score))\n",
    "    print(\"- F1 Score: {:.4f}\".format(model_train_f1_score))\n",
    "    print(\"- gini Score: {:.4f}\".format(model_train_gini))\n",
    "    print('----------------------------------')\n",
    "\n",
    "    print('Model Performance for Test Set:')\n",
    "    print(\"- Accuracy: {:.4f}\".format(model_test_accuracy_score))\n",
    "    print(\"- Precision: {:.4f}\".format(model_test_precision_score))\n",
    "    print(\"- F1 Score: {:.4f}\".format(model_test_f1_score))\n",
    "    print(\"- gini Score: {:.4f}\".format(model_test_gini))\n",
    "\n",
    "    accuracy_score_list.append(model_test_accuracy_score)  # Store test accuracy score\n",
    "\n",
    "    print('='*40)  # Separator for readability\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c5939c-1c31-4206-9fa0-a861a79c7fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the model names and their corresponding accuracy scores, \n",
    "# and sort the results in descending order based on the accuracy score\n",
    "result = pd.DataFrame(list(zip(model_list, accuracy_score_list)), columns=['Model Name', 'Accuracy_score']).sort_values(by=[\"Accuracy_score\"],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc1cd12-277b-49e3-94a0-d0dd6b3bbb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6b96e-eed4-4c36-8471-0847cc3fc077",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.Accuracy_score = result.Accuracy_score*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f67b14-d5a1-4627-82b5-d7acdc5479b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badaff93-f503-473f-be77-80996002cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) # plotting using matplotlib\n",
    "num = 0\n",
    "for i in result :\n",
    "    if num <= 7:\n",
    "        a = sns.barplot(x='Model Name', y='Accuracy_score', data=result,palette='viridis')\n",
    "        plt.title('R² Score Comparison of Models') # plotting using matplotlib\n",
    "        plt.xlabel('Model Name') # plotting using matplotlib\n",
    "        plt.ylabel('Accuracy score') # plotting using matplotlib\n",
    "        # plt.ylim(0, 1.1) # plotting using matplotlib\n",
    "        plt.xticks(rotation=90) # plotting using matplotlib\n",
    "        \n",
    "        for index,value in enumerate(result['Accuracy_score']):\n",
    "            a.text(index,value + 0.02 , f'{value:.2f}',ha='center')\n",
    "\n",
    "        num += 1\n",
    "\n",
    "plt.tight_layout() # plotting using matplotlib\n",
    "plt.show() # plotting using matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a57f85-9d64-4a17-bdbc-2357abe8f19f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Top Performers:\n",
    "- **Random Forest**: 96.31%\n",
    "- **Logistic Regression**, **Support Vector Machine**, **Linear SVM**, **AdaBoost**, and **CatBoost** — all scoring **96.30%**.\n",
    "- These models show almost identical performance based on the R² score.\n",
    "- **K Nearest Neighbors**: 96.25%\n",
    "- **XGBoost**: 96.25%\n",
    "- **Gradient Boosting**: 96.20%\n",
    "- **MLP Classifier (Neural Network)**: 95.58%\n",
    "\n",
    "- **Decision Tree**: 91.13% (significantly lower than the others)\n",
    "\n",
    "### Observations:\n",
    "- Most models are performing very similarly, clustering around **96% accuracy**.\n",
    "- The **Decision Tree** stands out as the weaker model in this comparison.\n",
    "- Ensemble methods like **Random Forest**, **AdaBoost**, **CatBoost**, **XGBoost**, and **Gradient Boosting** mostly perform well and consistently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f5877-d668-47ed-a333-3fb60de6029a",
   "metadata": {},
   "source": [
    "# Report on challange faced :-\n",
    "\n",
    "### 1. Data Quality Issues\n",
    "- **Missing Values**: Several fields like `paymenthistory2`, `feature_8`, `feature_9`, etc. have high missingness.\n",
    "- **Mixed Data Types**: Columns such as `opened_dt`, `last_paymt_dt`, `rateofinterest` contain mixed types (string + numeric).\n",
    "- **Inconsistent Formats**: Date fields (`opened_dt`, `enquiry_dt`, `closed_dt`) are not standardized and need transformation.\n",
    "\n",
    "### 2. Data Redundancy & Irrelevance\n",
    "- Presence of **ID-like variables** (`customer_no`, `upload_dt`) which do not contribute to prediction.\n",
    "- **Highly correlated features** (e.g., `cur_balance_amt` vs. `high_credit_amt`) create redundancy.\n",
    "- Some features may be **constant or near-constant**, adding no predictive value.\n",
    "\n",
    "### 3. Class Imbalance\n",
    "- Target variable `Bad_label` likely has **fewer bad customers (1)** compared to good ones (0).\n",
    "- Imbalanced datasets lead to biased models that predict mostly the majority class.\n",
    "\n",
    "### 4. Privacy & Feature Obfuscation\n",
    "- Demographic variables (`feature_1` to `feature_79`) are anonymized.\n",
    "- Lack of semantic meaning makes interpretation and business validation difficult.\n",
    "\n",
    "### 5. Data Integration Challenges\n",
    "- Data comes from **multiple sources** (Accounts, Enquiries, Demographics).\n",
    "- Requires **joining/merging on customer_no**; mismatches or missing keys may cause data loss.\n",
    "\n",
    "### 6. Outliers & Skewness\n",
    "- Financial features (`cur_balance_amt`, `enq_amt`, `high_credit_amt`) show **large ranges and skewed distributions**.\n",
    "- Outliers may distort model performance if not handled.\n",
    "\n",
    "### 7. Temporal Issues\n",
    "- Payment history and enquiry data are **time-dependent**.\n",
    "- Model may overfit if recency trends are not captured properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf86327-00cb-4140-99f6-feb870c88f9d",
   "metadata": {},
   "source": [
    "## Model Save :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00667cad-ab35-4324-989b-515d5eca2aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle                             #Importing the pickle module\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(RandomForestClassifier(), file)                      #save it to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c909d74-70f2-4bb1-a2e5-6f84df017286",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"best_model.pkl\",\"rb\") as f:   #Open the file \"best_model.pkl\" in read-binary mode\n",
    "  model=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef408ca-3e7e-4134-8973-36954868d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a49ef-92f6-405d-96cf-414004065499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86fae875-67ff-4ba1-86c1-f421d3627087",
   "metadata": {},
   "source": [
    "                                                      ## END ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
